#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
RAG Job Matcher - å®Œæ•´ç‰ˆæœ¬ï¼ˆå®˜æ–¹ OpenAI API è°ƒç”¨ + æœ¬åœ° HuggingFace embeddings + Chromaï¼‰
è¯´æ˜:
  - è¯·åœ¨ç»ˆç«¯è®¾ç½® OPENAI_API_KEYï¼ˆæ¨èï¼‰: e.g.
      PowerShell: $env:OPENAI_API_KEY="sk-..."
      mac/linux: export OPENAI_API_KEY="sk-..."
  - è‹¥ä½¿ç”¨ä»£ç†æˆ– openrouterï¼Œè¯·åŒæ—¶è®¾ç½® OPENAI_API_BASEã€‚
  - éœ€è¦æœ¬åœ° embeddings ç›¸å…³ä¾èµ–ï¼š sentence-transformers transformers huggingface-hub
  - å…ˆç¡®ä¿å·²æ„å»ºå¥½ chroma_storeï¼ˆprepare_data.py / build_vectorstore.pyï¼‰
ç”¨æ³•:
  python rag_job_matcher.py --n 10 --model gpt-3.5-turbo --topk 3
"""

import os
import sys
import json
import random
import argparse
import traceback
from pathlib import Path
from typing import Dict, List, Any

from rich.console import Console
from rich.table import Table

console = Console()

# ---------------- default config ----------------
DEFAULT_EMBEDDING_MODEL = "sentence-transformers/all-MiniLM-L6-v2"
DEFAULT_MODEL = os.getenv("MODEL_NAME", "gpt-3.5-turbo")
DEFAULT_TEMPERATURE = float(os.getenv("TEMPERATURE", "0.2"))
DEFAULT_CHROMA_DIR = Path("chroma_store")
DEFAULT_COLLECTION = "job-postings"
DEFAULT_DATA_DIR = Path("data")
DEFAULT_DATA_DIR.mkdir(exist_ok=True)
# -------------------------------------------------

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OPENAI_API_BASE = os.getenv("OPENAI_API_BASE")  # optional

# ---------------- helpers: candidate gen ----------------
def generate_candidates(n: int = 5) -> List[Dict[str, Any]]:
    """éšæœº/ä¼ªé€ ä¸€æ‰¹å€™é€‰äººï¼ˆç”¨äºæµ‹è¯•ï¼‰"""
    SKILLS = [
        "Python", "Pandas", "SQL", "Tableau", "PowerBI", "A/B Test",
        "æœºå™¨å­¦ä¹ ", "NLP", "Spark", "Docker", "Kubernetes", "Java",
        "Golang", "React", "æ•°æ®å¯è§†åŒ–", "ç»Ÿè®¡åˆ†æ", "Excel"
    ]
    CITIES = ["ä¸Šæµ·", "åŒ—äº¬", "æ·±åœ³", "å¹¿å·", "æ­å·", "æˆéƒ½", "å—äº¬", "è‹å·"]

    # å°è¯•ç”¨ Faker ç”Ÿæˆä¸­æ–‡åå­—ï¼Œfallback éšæœºç»„åˆ
    names = []
    try:
        from faker import Faker
        fake = Faker("zh_CN")
        for _ in range(n):
            names.append(fake.name())
    except Exception:
        surnames = ["å¼ ", "ç‹", "æ", "èµµ", "åˆ˜", "é™ˆ", "æ¨", "é»„", "å‘¨", "å´"]
        given = ["é›…", "ç³", "å¼º", "æ˜", "å©·", "ç£Š", "è¶…", "æ°", "å", "æ–Œ", "ç‡•", "è½©", "æµ©"]
        for _ in range(n):
            names.append(random.choice(surnames) + random.choice(given))

    objectives = [
        "åŠ å…¥äº’è”ç½‘/é›¶å”®è¡Œä¸šæ•°æ®å›¢é˜Ÿï¼Œå…³æ³¨å¢é•¿åˆ†æä¸æ•°æ®é©±åŠ¨å†³ç­–",
        "æˆä¸ºäº§å“æ•°æ®åˆ†æå¸ˆï¼Œæå‡äº§å“è½¬åŒ–ç‡å’Œç”¨æˆ·ç•™å­˜",
        "ä»äº‹æ•°æ®å·¥ç¨‹/åç«¯å·¥ä½œï¼Œä¼˜åŒ–æ•°æ®æµæ°´çº¿ä¸æ€§èƒ½",
        "åœ¨é‡‘èæ•°æ®å›¢é˜Ÿä»äº‹é£æ§/ç‰¹å¾å·¥ç¨‹å·¥ä½œ",
    ]

    candidates = []
    for i in range(n):
        cand = {
            "name": names[i],
            "city_preference": random.choice(CITIES),
            "years_experience": random.choice([1, 2, 3, 4, 5, 6]),
            "skills": random.sample(SKILLS, k=random.randint(3, 6)),
            "objective": random.choice(objectives),
        }
        candidates.append(cand)
    return candidates

def format_profile(candidate: Dict[str, Any]) -> str:
    skills = "ã€".join(candidate.get("skills", []))
    profile = f"""å§“åï¼š{candidate.get('name')}
ç›®æ ‡åŸå¸‚ï¼š{candidate.get('city_preference')}ä¼˜å…ˆ
å·¥ä½œç»éªŒï¼š{candidate.get('years_experience')} å¹´
æŠ€èƒ½æ ‡ç­¾ï¼š{skills}
èŒä¸šç›®æ ‡ï¼š{candidate.get('objective')}
"""
    return profile.strip()

# ---------------- embeddings + chroma loader ----------------
def load_hf_embeddings(local_model_name: str = DEFAULT_EMBEDDING_MODEL):
    """å…¼å®¹ä¸åŒç‰ˆæœ¬çš„ langchain å¯¼å…¥ HuggingFaceEmbeddings"""
    console.print(f"[green]ä½¿ç”¨æœ¬åœ° embeddingsï¼ˆç”¨äºæŸ¥è¯¢ï¼‰ï¼š{local_model_name}[/green]")
    try:
        from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings  # type: ignore
    except Exception:
        try:
            from langchain.embeddings import HuggingFaceEmbeddings  # type: ignore
        except Exception as e:
            console.print("[red]æ— æ³•å¯¼å…¥ HuggingFaceEmbeddingsï¼Œè¯·å®‰è£… sentence-transformers ä¸ langchain_community æˆ– langchainã€‚[/red]")
            raise e
    return HuggingFaceEmbeddings(model_name=local_model_name)

def load_chroma_vectorstore(embedding_function, chroma_dir: Path = DEFAULT_CHROMA_DIR, collection_name: str = DEFAULT_COLLECTION):
    """åŠ è½½å·²æŒä¹…åŒ–çš„ Chroma å‘é‡åº“ï¼Œå¹¶ä¼ å…¥ embedding_functionï¼ˆæŸ¥è¯¢æ—¶éœ€è¦ï¼‰"""
    try:
        # æ–°ç‰ˆå»ºè®®ä½¿ç”¨ langchain_community.vectorstores.Chroma
        from langchain_community.vectorstores import Chroma  # type: ignore
    except Exception:
        try:
            from langchain.vectorstores import Chroma  # fallback
        except Exception as e:
            console.print("[red]æ— æ³•å¯¼å…¥ Chromaï¼Œè¯·å®‰è£… chromadb / langchain_communityã€‚[/red]")
            raise e

    vectorstore = Chroma(
        persist_directory=str(chroma_dir),
        collection_name=collection_name,
        embedding_function=embedding_function,
    )
    return vectorstore

# ---------------- retrieve top-k ----------------
def retrieve_topk(query: str, k: int = 3, embedding_model: str = DEFAULT_EMBEDDING_MODEL, chroma_dir: Path = DEFAULT_CHROMA_DIR, collection_name: str = DEFAULT_COLLECTION):
    """ä½¿ç”¨æœ¬åœ° embeddings ä» chroma_store æ£€ç´¢ top-k æ–‡æ¡£"""
    try:
        hf = load_hf_embeddings(local_model_name=embedding_model)
        vectorstore = load_chroma_vectorstore(embedding_function=hf, chroma_dir=chroma_dir, collection_name=collection_name)
    except Exception as e:
        console.print("[red]åŠ è½½å‘é‡åº“æˆ– embeddings å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä¾èµ–ä¸ chroma_store æ˜¯å¦å­˜åœ¨ã€‚[/red]")
        raise

    retriever = vectorstore.as_retriever(search_kwargs={"k": k})
    # å…¼å®¹æ–°æ—§æ¥å£
    try:
        if hasattr(retriever, "get_relevant_documents"):
            docs = retriever.get_relevant_documents(query)
        elif hasattr(retriever, "invoke"):
            docs = retriever.invoke(query)
        else:
            # fallback to similarity_search on vectorstore (embedding_function must be present)
            docs = vectorstore.similarity_search(query, k=k)
    except Exception as e:
        # å°è¯•ç›´æ¥ similarity_searchï¼ˆembedding_function å¿…é¡»åœ¨ Chroma ä¸­ï¼‰
        try:
            docs = vectorstore.similarity_search(query, k=k)
        except Exception as e2:
            console.print("[red]æ£€ç´¢å¤±è´¥ï¼ˆå·²å°è¯•å¤šç§æ–¹æ³•ï¼‰ï¼š[/red]")
            traceback.print_exc()
            raise RuntimeError(f"æ£€ç´¢å¤±è´¥: {e} / {e2}") from e2
    return docs

# ---------------- pretty print sources ----------------
def pretty_print_sources(sources: List[Any]):
    table = Table(title="ğŸ” å‘½ä¸­å²—ä½æ–‡æ¡£ï¼ˆTop Kï¼‰", show_lines=True)
    table.add_column("å²—ä½ ID", style="cyan", no_wrap=True)
    table.add_column("å²—ä½åç§°", style="magenta")
    table.add_column("å…¬å¸", style="green")
    table.add_column("åŸå¸‚", style="yellow")

    for d in sources:
        meta = getattr(d, "metadata", None) or (d.get("metadata") if isinstance(d, dict) else {})
        job_id = meta.get("job_id") or meta.get("id") or "-"
        table.add_row(str(job_id), meta.get("title", "-"), meta.get("company", "-"), meta.get("location", meta.get("city", "-")))
    console.print(table)

# ---------------- robust OpenAI chat call ----------------
def call_openai_chat(messages: List[Dict[str, str]], model: str = DEFAULT_MODEL, temperature: float = DEFAULT_TEMPERATURE, max_tokens: int = 800) -> str:
    """
    æ›´é²æ£’çš„ OpenAI chat è°ƒç”¨ï¼šå…ˆå°è¯•è€æ¥å£ openai.ChatCompletion.createï¼Œå†å°è¯•æ–°ç‰ˆ openai.OpenAI å®¢æˆ·ç«¯ã€‚
    ä¼šå°½å¯èƒ½è§£æå¹¶è¿”å›æ–‡æœ¬å†…å®¹ï¼›è‹¥éƒ½å¤±è´¥ä¼šæŠ›å‡ºå¼‚å¸¸å¹¶å°½é‡æ‰“å° raw response ä¾›æ’æŸ¥ã€‚
    """
    raw_debug = None
    # 1) æ—§é£æ ¼ openai åŒ…çš„ ChatCompletion APIï¼ˆå¾ˆå¤šé¡¹ç›®é‡Œä»å¯å·¥ä½œï¼‰
    try:
        import openai
        if OPENAI_API_KEY:
            openai.api_key = OPENAI_API_KEY
        if OPENAI_API_BASE:
            openai.api_base = OPENAI_API_BASE

        resp = openai.ChatCompletion.create(model=model, messages=messages, temperature=temperature, max_tokens=max_tokens)
        raw_debug = resp
        # è§£ææœ€å¸¸è§ç»“æ„
        if isinstance(resp, dict) and "choices" in resp and resp["choices"]:
            choice = resp["choices"][0]
            # ç°ä»£ ChatCompletion è¿”å› choices[0]["message"]["content"]
            if isinstance(choice, dict) and "message" in choice and isinstance(choice["message"], dict) and "content" in choice["message"]:
                return choice["message"]["content"]
            # older style might have 'text'
            if isinstance(choice, dict) and "text" in choice:
                return choice["text"]
        # object style
        if hasattr(resp, "choices"):
            choices = getattr(resp, "choices")
            if choices and len(choices) > 0:
                c0 = choices[0]
                # c0 may have .message.content
                msg = getattr(c0, "message", None)
                if msg and getattr(msg, "content", None):
                    return msg.content
                text = getattr(c0, "text", None)
                if text:
                    return text
        # è‹¥æœªè§£æåˆ™ç»§ç»­åˆ°æ–°ç‰ˆ client
    except Exception as e_old:
        console.print(f"[grey]debug: openai.ChatCompletion.create failed or returned odd structure: {e_old}[/grey]")

    # 2) æ–° openai client (openai>=1.0) çš„ç”¨æ³•
    try:
        from openai import OpenAI
        client = None
        try:
            client = OpenAI(api_key=OPENAI_API_KEY) if OPENAI_API_KEY else OpenAI()
            # æ‚¨ä¹Ÿå¯ä»¥é€šè¿‡ç¯å¢ƒå˜é‡ OPENAI_API_BASE æ§åˆ¶ base urlï¼ˆOpenRouter ç­‰ï¼‰
        except Exception:
            client = OpenAI()

        resp2 = client.chat.completions.create(model=model, messages=messages, temperature=temperature, max_tokens=max_tokens)
        # å°è¯•è§£æ resp2ï¼ˆå¯èƒ½æ˜¯ OpenAI SDK çš„å¯¹è±¡ï¼‰
        try:
            d = resp2 if isinstance(resp2, dict) else (resp2.to_dict() if hasattr(resp2, "to_dict") else dict(resp2.__dict__))
        except Exception:
            d = str(resp2)

        # å°è¯•æ‰“å°éƒ¨åˆ† raw response ä¾¿äºè°ƒè¯•ï¼ˆä¸ä¼šå¤ªé•¿ï¼‰
        try:
            console.print("[grey]DEBUG raw OpenAI.chat.completions response (truncated):[/grey]")
            console.print(json.dumps(d, default=str, ensure_ascii=False)[:2000])
        except Exception:
            console.print("[grey]DEBUG raw OpenAI response (non-json) (truncated).[/grey]")

        # è§£æ d
        if isinstance(d, dict) and "choices" in d and d["choices"]:
            ch = d["choices"][0]
            if isinstance(ch, dict):
                if "message" in ch and isinstance(ch["message"], dict) and "content" in ch["message"]:
                    return ch["message"]["content"]
                if "text" in ch:
                    return ch["text"]
        # attributes fallback
        if hasattr(resp2, "choices") and resp2.choices:
            c0 = resp2.choices[0]
            if hasattr(c0, "message") and getattr(c0.message, "content", None):
                return c0.message.content
            if hasattr(c0, "text") and getattr(c0, "text", None):
                return c0.text

        raise RuntimeError("æ— æ³•è§£æ OpenAI å®¢æˆ·ç«¯è¿”å›çš„å“åº”ç»“æ„ï¼ˆå·²æ‰“å° raw responseï¼‰")
    except Exception as e_new:
        # è‹¥ä¸¤ç§æ–¹æ³•éƒ½å¤±è´¥ï¼Œç»™å‡ºæç¤ºä¿¡æ¯ï¼ˆé€šå¸¸æ˜¯ key/endpoint é—®é¢˜ï¼‰
        raise RuntimeError(f"OpenAI è°ƒç”¨å¤±è´¥ï¼ˆè¯·ç¡®è®¤ OPENAI_API_KEY/OPENAI_API_BASE æ˜¯å¦æ­£ç¡®ï¼Œæˆ– provider çŠ¶æ€ï¼‰ï¼š{e_new}") from e_new

# ---------------- build prompt from docs ----------------
def build_prompt_from_docs(candidate_profile: str, docs: List[Any]) -> List[Dict[str, str]]:
    ctx_parts = []
    for d in docs:
        content = getattr(d, "page_content", "") or (d.get("page_content") if isinstance(d, dict) else "")
        meta = getattr(d, "metadata", None) or (d.get("metadata") if isinstance(d, dict) else {})
        header = f"å²—ä½ID: {meta.get('job_id','-')} | æ ‡é¢˜: {meta.get('title','-')} | å…¬å¸: {meta.get('company','-')} | åŸå¸‚: {meta.get('location','-')}"
        ctx_parts.append(header + "\n" + (content.strip() if content else ""))

    context_text = "\n\n---\n\n".join(ctx_parts)

    prompt_template = """ä½ æ˜¯ä¸€ä½èµ„æ·± HR é¡¾é—®ï¼Œæ“…é•¿æ ¹æ®å€™é€‰äººçš„æŠ€èƒ½èƒŒæ™¯æ¨èåŒ¹é…å²—ä½ã€‚
ç»“åˆæä¾›çš„ã€å€™é€‰äººä¿¡æ¯ã€‘å’Œæ£€ç´¢åˆ°çš„ã€å²—ä½ä¿¡æ¯ã€‘ï¼Œè¯·è¾“å‡ºï¼š
1. åŒ¹é…åº¦è¯„åˆ†ï¼ˆç™¾åˆ†åˆ¶ï¼Œç®€çŸ­ç†ç”±ï¼‰
2. æ¨èå²—ä½åˆ—è¡¨ï¼ˆæ¯ä¸ªå²—ä½åŒ…å«ï¼šå²—ä½åç§° / å…¬å¸ / åŸå¸‚ / æ¨èç†ç”±ï¼‰
3. é’ˆå¯¹å€™é€‰äººçš„è¡¥å……å»ºè®®ï¼ˆå¦‚éœ€åŠ å¼ºçš„æŠ€èƒ½ã€é¡¹ç›®ç»å†ç­‰ï¼‰

è¾“å‡ºæ ¼å¼ä½¿ç”¨ Markdownã€‚

æ£€ç´¢åˆ°çš„å²—ä½ä¿¡æ¯ï¼š
{context}

å€™é€‰äººä¿¡æ¯ï¼š
{candidate}

è¯·ç»™å‡ºæ¨èç»“æœï¼ˆç®€æ´ã€æ¡ç†æ¸…æ™°ï¼‰ã€‚"""

    user_prompt = prompt_template.format(context=context_text, candidate=candidate_profile.strip())

    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„åŠ©ç†ï¼Œå›ç­”æ—¶è¦ç®€æ´å¹¶ç»™å‡ºæ¸…æ™°å»ºè®®ã€‚"},
        {"role": "user", "content": user_prompt},
    ]
    return messages

# ---------------- main batch runner ----------------
def run_batch(n_candidates: int = 5, model: str = DEFAULT_MODEL, topk: int = 3, embedding_model: str = DEFAULT_EMBEDDING_MODEL):
    # ç”Ÿæˆå€™é€‰äººå¹¶ä¿å­˜
    candidates = generate_candidates(n_candidates)
    cand_file = DEFAULT_DATA_DIR / "candidates.json"
    with open(cand_file, "w", encoding="utf-8") as f:
        json.dump(candidates, f, ensure_ascii=False, indent=2)
    console.print(f"[green]å·²ç”Ÿæˆå€™é€‰äººå¹¶ä¿å­˜ï¼š{cand_file}[/green]")

    results = []

    for idx, cand in enumerate(candidates, start=1):
        console.rule(f"[bold blue]å€™é€‰äºº {idx}/{len(candidates)}ï¼š{cand['name']}")
        profile_text = format_profile(cand)
        console.print(profile_text)

        console.rule("[bold cyan]æ£€ç´¢ Top-K å²—ä½")
        try:
            docs = retrieve_topk(profile_text, k=topk, embedding_model=embedding_model)
            pretty_print_sources(docs)
        except Exception as e:
            console.print("[red]æ£€ç´¢å¤±è´¥ï¼šè·³è¿‡è¯¥å€™é€‰äººå¹¶è®°å½•é”™è¯¯ã€‚[/red]")
            results.append({"candidate": cand, "error": str(e)})
            continue

        console.rule("[bold magenta]è°ƒç”¨æ¨¡å‹ç”Ÿæˆæ¨èï¼ˆOpenAI Chatï¼‰")
        try:
            messages = build_prompt_from_docs(profile_text, docs)
            reply = call_openai_chat(messages=messages, model=model, temperature=DEFAULT_TEMPERATURE, max_tokens=800)
            console.print("\n[bold green]æ¨¡å‹å›å¤ï¼š[/bold green]\n")
            console.print(reply)
            results.append({
                "candidate": cand,
                "profile_text": profile_text,
                "rag_answer": reply,
                "sources_meta": [ (getattr(d, "metadata", None) or (d.get("metadata") if isinstance(d, dict) else {})) for d in docs ]
            })
        except Exception as e:
            console.print("[red]è°ƒç”¨æ¨¡å‹å¤±è´¥ï¼šè®°å½•é”™è¯¯å¹¶ç»§ç»­ã€‚[/red]")
            traceback.print_exc()
            results.append({"candidate": cand, "error": str(e)})

    out_file = DEFAULT_DATA_DIR / "results.json"
    with open(out_file, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    console.print(f"[green]å…¨éƒ¨å®Œæˆã€‚ç»“æœå·²ä¿å­˜ï¼š{out_file.resolve()}[/green]")

# ---------------- CLI ----------------
def parse_args():
    p = argparse.ArgumentParser()
    p.add_argument("--n", type=int, default=5, help="ç”Ÿæˆå€™é€‰äººäººæ•°")
    p.add_argument("--model", type=str, default=DEFAULT_MODEL, help="OpenAI æ¨¡å‹åï¼ˆgpt-3.5-turbo/gpt-4 ç­‰ï¼‰")
    p.add_argument("--topk", type=int, default=3, help="æ£€ç´¢ Top-K æ–‡æ¡£")
    p.add_argument("--embedding", type=str, default=DEFAULT_EMBEDDING_MODEL, help="æœ¬åœ° embedding æ¨¡å‹")
    return p.parse_args()

if __name__ == "__main__":
    args = parse_args()
    if not OPENAI_API_KEY:
        console.print("[yellow]è­¦å‘Šï¼šå½“å‰æœªæ£€æµ‹åˆ° OPENAI_API_KEY ç¯å¢ƒå˜é‡ã€‚è‹¥è¦ä½¿ç”¨å®˜æ–¹ OpenAIï¼Œè¯·å…ˆåœ¨ç»ˆç«¯å¯¼å‡ºæˆ–åœ¨ .env ä¸­è®¾ç½®ã€‚[/yellow]")
    run_batch(n_candidates=args.n, model=args.model, topk=args.topk, embedding_model=args.embedding)
